## **一、综述**

**【接入层】**

-   反向代理

-   -   [nginx异步非阻塞IO密集型，适合做反向代理。apache每个请求独占一个线程，适合做webserver。]
    -   [webserver中间件：nginx（c）、OpenResty（lua）、apache、tomcat、jetty、tengine]

-   负载均衡（一般互联网公司使用DNS做一级负载均衡，后端服务器用Nginx或LVS等作为二级负载均衡。）

-   -   LVS：Linux Virtual Server
    -   F5：硬件设备
    -   DNS：DNS服务商配置，多级缓存，更新慢
    -   Nginx：自带多种负载均衡策略：轮询(Round-Robin)、最少连接(Least-connected)、最少耗时(Least-time)、会话保持(session persistence)、自定义hash、权重。
    -   ...

-   限流

-   -   简单计数法
    -   滑动窗口法（滑动统计最近一段时间的请求量。精度提升）
    -   令牌桶算法（系统以恒定的速度向桶里放令牌）
    -   漏桶算法（漏桶以一定的速度出水）

-   降级

-   熔断：快速失败，不会真正去请求外部资源。

-   超时

-   防雪崩
    [容错组件：Hystrix（netflix，java，封装了熔断、限流、隔离、降级等能力）]



**【逻辑层】**

-   可扩展

-   -   水平扩展（冗余部署，防单点）
    -   垂直扩展（提升处理能力，增加内存磁盘等）

-   隔离：避免单一业务占用全部资源；避免业务之间的相互影响 。

-   解耦：剥离非核心业务，如用异步消息解耦。

-   RPC

-   -   [RPC框架：dubbo（阿里java框架）、thrift（apache开源）、gRPC、rpcx（go版dubbo）]
    -   [微服务restful框架：Spring Boot/Cloud（netflix）]

-   同步、异步

-   连接池

-   消息队列

-   -   [消息队列中间件：RabbitMQ、RocketMQ、ActiveMQ、Kafka、ZeroMQ、Redis消息队列]

-   延迟计算

-   数据预读

-   轮询通知

-   事件触发器

-   模块化



**【数据层】**

-   缓存

-   -   缓存穿透（db也不存在）

    -   -   布隆过滤器（bloomfilter）；
        -   缓存NULL。

    -   缓存雪崩（key集体过期）

    -   -   缓存失效时间增加小随机数，避免集体失效；
        -   永不过期，旁路进程扫描替换过期数据。

    -   缓存击穿（单热点key）

    -   -   分散热点key为多个子key，对应相同的value存储在不同服务器上，分散单机压力；
        -   （针对单机的多进程）单进程更新缓存，加分布式锁，其他进程等待，避免数据过期时恰有大量的并发请求压向存储系统；
        -   （针对单机的多进程）“提前”使用互斥锁：内部超时时间（小于实际超时时间）一到，马上更新超时时间（其他进程则认为没过期）并加载db缓存。

[本地缓存中间件：guava cache（Google开源）]

[服务端缓存中间件：memcached、redis、Tair（淘宝开源分布式kv缓存系统）]

-   一致性

-   -   CAP：P分区容错性(网络故障引起)是分布式系统最基本的要求，因此往往需要把精力花在如何根据业务特点在C(一致性)和A(可用性)之间寻求平衡。

    -   分布式锁

    -   -   数据库分布式锁：简单易理解；单点、性能差、不可重入。
        -   缓存分布式锁：非阻塞性能好；复杂容易死锁。
        -   Zookeeper分布式锁：通过有序临时节点实现锁机制。
        -   redis分布式锁：基于setnx(set if not exists)，单进程单线程模式不用加锁。

    -   一致性算法

    -   -   Paxos
        -   Zab（Zookeeper中的分布式一致性算法）
        -   Raft（etcd使用raft协议）：正确高效简洁。通过随机等待的方式发出投票，的票多者获胜。
        -   Gossip

    -   幂等（重复调用的结果相同），常见手段：悲观锁、乐观锁、唯一索引、一次性token、序列号等。

-   主从架构

-   -   一般主从系统都会选择强一致协议，即主节点采用将数据发送给从节点收到响应成功后，才会将数据持久化到本地，返回用户成功。

    -   单点问题（应尽量减少与单点的交互。两种常见方法：批量写 和 客户端缓存。）

    -   -   双主架构
        -   影子master架构
        -   中心调度节点通常直接用zookeeper消灭单点。

-   去中心化架构

-   -   Quorum协议，多数派原则。

-   唯一ID生成：核心思想是结合服务/机器、时间、随机数来组合生成UUID。

-   一致性hash：确保在缓存服务器节点数量发生变化时大部分数据保持原地不动。

-   数据库扩展

-   -   通常mysql单库5千万条需要分库（最好能拆分到1千万以内）。
    -   问题：事务、Join、数据迁移、容量规划和扩容、ID路由、分页等。
    -   [分库分表中间件：sharding-jdbc(当当)、Tsharding(蘑菇街)、Atlas(奇虎360)、MyCAT(阿里)、Vitess(Google)等。]



**【服务发现】**

-   服务注册与发现

[服务注册与发现中间件：zookeeper、etcd、consul、euerka（netflix开源）等。]

-   服务路由
-   服务网关

[API网关：Zuul（配套Spring Cloud）、Kong（配套Nngix/OpenResty）]

-   配置中心

[配置中心中间件：Apollo（携程开源，支持配置版本管理和灰度发布）]



**【综合】**

-   监控

-   -   系统监控
    -   调用质量监控
    -   数据库监控
    -   应用逻辑监控
    -   告警
    -   [日志监控中间件：ELK、CAT（大众点评开源，调用链监控）、Zipkin（调用链监控）]

-   日志

-   -   染色日志
    -   [分布式日志中间件：Dapper，大规模分布式系统的跟踪系统]

-   测试

-   -   单元测试
    -   性能测试
    -   自动化测试
    -   真实流量测试

-   灰度发布

-   平滑启动：端流量、flush数据、重启应用。

-   异地多活

-   容器docker

## **二、一些细节补充**

主要是给解决主库单点故障这个问题提供一个思路。

### **影子****master****架构**

shadow-master是一种很常见的解决单点高可用问题的技术方案。

业内经常使用keepalived + vip的方式实现这类单点的高可用。

![MySQL双主架构](assets/image2019-10-11_23-4-25.png)

### **双主架构**

![MySQL双主架构](assets/image2019-10-11_23-5-3.png)

1.一台主库(我们称之为master-01)提供服务，只负责数据的写入；

2.拿出一台数据库服务器(我们称之为master-02)资源做master-01主库的从库(之间做主从同步)；

3.两台主库之间做高可用,可以采用keepalived等方案(一定要保证master-01同时也要作为keepalived的主)

4.程序在调用主库IP地址的地方写为高可用的VIP地址；

5.所有提供服务的从服务器与master-02进行主从同步；

6.建议采用高可用策略的时候，当master-01出现问题切换到master-02的时候，即使master-01恢复了，也不要让它去自动承接VIP地址，否则可能造成数据的混写；



### **熔断**

异常情况超出阈值进入熔断状态，服务调用不会真正去请求外部资源，快速失败。减少不稳定的外部依赖对核心服务的影响。

![熔断](assets/image2019-10-11_23-5-44.png)

当熔断器开关处于打开状态， 经过一段时间后, 熔断器会自动进入半开状态， 这时熔断器只允许一个请求通过。当该请求调用成功时，熔断器恢复到关闭状态。 若该请求失败， 熔断器继续保持打开状态， 接下来的请求被禁止通过。

**熔断开关：**服务的健康状况 = 请求失败数 / 请求总数，通过阈值设定和滑动窗口控制开关。



### **Zookeeper** **分布式锁**

Zookeeper致力于提供一个高性能、高可用、且具有严格的顺序访问控制能力（主要是写操作的严格顺序性）的分布式协调服务。分布式应用程序可以基于它实现诸如数据发现/发布、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。

Zookeeper引入了Leader、Follower、Observer三种角色，Zookeeper集群中的所有机器通过Lease选举过程来选定一台被称为Leader的机器，Leader服务器为客户端提供写服务，Follower和Observer提供读服务，但是Observer不参与Leader选举过程，不参与写操作的过半写成功策略，Observer可以在不影响写性能的情况下提升集群的性能。

客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的zookeeper机器来处理。对于写请求，这些请求会同时发给其他zookeeper机器并且达成一致后，请求才会返回成功。因此，随着zookeeper的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。

通过有序临时节点实现锁机制，自己对应的节点序号最小，则被认为是获得了锁。

优点：集群可以透明解决单点问题，避免锁不被释放问题，同时锁可以重入。

缺点：性能不如缓存方式，吞吐量会随着zk集群规模变大而下降。

zookeeper分布式锁大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。

看下Zookeeper能不能解决前面提到的问题。

-   **锁无法释放？**使用Zookeeper可以有效的解决锁无法释放的问题，因为在创建锁的时候，客户端会在ZK中创建一个临时节点，一旦客户端获取到锁之后突然挂掉（Session连接断开），那么这个临时节点就会自动删除掉。其他客户端就可以再次获得锁。
-   **非阻塞锁？**使用Zookeeper可以实现阻塞的锁，客户端可以通过在ZK中创建顺序节点，并且在节点上绑定监听器，一旦节点有变化，Zookeeper会通知客户端，客户端可以检查自己创建的节点是不是当前所有节点中序号最小的，如果是，那么自己就获取到锁，便可以执行业务逻辑了。
-   **不可重入？**使用Zookeeper也可以有效的解决不可重入的问题，客户端在创建节点的时候，把当前客户端的主机信息和线程信息直接写入到节点中，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。
-   **单点问题？**使用Zookeeper可以有效的解决单点问题，ZK是集群部署的，只要集群中有半数以上的机器存活，就可以对外提供服务。



### **etcd****是使用****raft****协议，类似****zookeeper****的分布式存储系统**

前者是一个用于存储关键数据的键值存储，后者是一个用于管理配置等信息的中心化服务。

我们依然可以使用 etcd 实现微服务架构中的服务发现、发布订阅、分布式锁以及分布式协调等功能，因为虽然它被定义成了一个可靠的分布式键值存储，但它起到的依然是一个分布式协调服务的作用，这使我们可以在不同的协调服务中进行权衡和选择。

为什么要在分布式协调服务中选择 etcd 其实是一个比较关键的问题，很多工程师选择 etcd 主要是因为它使用 Go 语言开发、部署简单、社区也比较活跃，但是缺点就在于它相比 Zookeeper 还是一个比较年轻的项目，需要一些时间来成长和稳定。

etcd 的实现原理非常有趣，我们能够在它的源代码中学习很多 Go 编程的最佳实践和设计，这也值得我们去研究它的源代码。



### **分布式消息队列**

以常见的订单系统为例，用户点击【下单】按钮之后的业务逻辑可能包括：扣减库存、生成相应单据、发红包、发短信通知。在业务发展初期这些逻辑可能放在一起同步执行，随着业务的发展订单量增长，需要提升系统服务的性能，这时可以将一些不需要立即生效的操作拆分出来异步执行，比如发放红包、发短信通知等。这种场景下就可以用 MQ ，在下单的主流程（比如扣减库存、生成相应单据）完成之后发送一条消息到 MQ 让主流程快速完结，而由另外的单独线程拉取MQ的消息（或者由 MQ 推送消息），当发现 MQ 中有发红包或发短信之类的消息时，执行相应的业务逻辑。

以上是用于业务解耦的情况，其它常见场景包括最终一致性、广播、错峰流控等等。

-   Push方式：优点是可以尽可能快地将消息发送给消费者，缺点是如果消费者处理能力跟不上，消费者的缓冲区可能会溢出。
-   Pull方式：优点是消费端可以按处理能力进行拉取，缺点是会增加消息延迟。

**RabbitMQ**

支持事务，推拉模式都是支持、适合需要可靠性消息传输的场景。非常重量级，更适合于企业级的开发。

**RocketMQ**

推拉模式都是支持，吞吐量逊于Kafka。可以保证消息顺序。

**ActiveMQ**

ActiveMQ是Apache下的一个子项目。类似于ZeroMQ，它能够以代理人和点对点的技术实现队列。

同时类似于RabbitMQ，它少量代码就可以高效地实现高级应用场景。

**Kafka**

高吞吐量、采用拉模式。适合高IO场景，比如日志同步。

Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。

**Redis**

使用Redis的List数据结构可以简单迅速地做一个消息队列。

Redis提供的BRPOP和BLPOP等指令解决了频繁调用Jedis的rpop和lpop方法造成的资源浪费问题。

Redis提供对发布/订阅模式的指令，可以实现消息传递、进程间通信。

入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；

出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。

**ZeroMQ**

ZeroMQ号称最快的消息队列系统，尤其针对大吞吐量的需求场景。

ZMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，技术上的复杂度是对ZMQ能够应用成功的挑战。ZeroMQ具有一个独特的非中间件的模式，你不需要安装和运行一个消息[服务器](https://www.baidu.com/s?wd=服务器&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)或中间件，因为你的应用程序将扮演了这个服务角色。你只需要简单的引用ZeroMQ程序库，可以使用NuGet安装，然后你就可以愉快的在应用程序之间发送消息了。但是ZeroMQ仅提供非持久性的队列，也就是说如果down机，数据将会丢失。其中，Twitter的Storm中默认使用ZeroMQ作为数据流的传输。

### **Lua****的特点**

-   Lua脚本可以很容易的被C/C++代码调用，也可以反过来调用C/C++的函数，这使得Lua在应用程序中可以被广泛应用。不仅仅作为扩展脚本，也可以作为普通的配置文件，代替XML,Ini等文件格式，并且更容易理解和维护。
-   Lua由标准C编写而成，代码简洁优美，几乎在所有操作系统和平台上都可以编译，运行。一个完整的Lua解释器不过200k，在目前所有脚本引擎中，Lua的速度是最快的。



### [**《****redis****底层原理》**](https://blog.csdn.net/wcf373722432/article/details/78678504)

-   使用 ziplist 存储链表，ziplist是一种压缩链表，它的好处是更能节省内存空间，因为它所存储的内容都是在连续的内存区域当中的。
-   使用 skiplist(跳跃表)来存储有序集合对象、查找上先从高Level查起、时间复杂度和红黑树相当，实现容易，无锁、并发性好。

### [**《****Redis****持久化方式》**](http://doc.redisfans.com/topic/persistence.html)

-   RDB方式：定期备份快照，常用于灾难恢复。优点：通过fork出的进程进行备份，不影响主进程、RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。缺点：会丢数据。
-   AOF方式：保存操作日志方式。优点：恢复时数据丢失少，缺点：文件大，恢复慢。
-   也可以两者结合使用。

### **Redis****经验分享**

-   Redis异步尽量不用，因为Redis延迟本身很小，大概在100us-200us，再一个就是Redis本身是单线程的，异步任务切换的耗时比网络耗时还要大。
-   在Linux上多实例部署，实例个数等于处理器个数，各实例最大内存直接为本机物理内存，避免单个实例内存撑爆（比方说8核心处理器，那么就部署8个实例）。
-   把海量数据（10亿+）根据key哈希（Crc16/Crc32）存放在多个实例上，读写性能成倍增长。
-   采用二进制序列化，而非常见的Json序列化。
-   合理设计每一对Key的Value大小，包括但不限于使用批量获取，原则是让每次网络包控制在1.4k字节附近，减少通信次数（实际经验几十k，几百k也是没问题的）。
-   Redis客户端的Get/Set操作平均耗时200~600us（含往返网络通信），以此为参考评估网络环境和Redis客户端组件（达不到就看一下网络，序列化方式等等）。
-   使用管道Pipeline合并一批命令。
-   Redis的主要性能瓶颈是序列化、网络带宽和内存大小，滥用时处理器也会达到瓶颈。
-   Redis实现ICache接口。它的孪生兄弟MemoryCache，内存缓存，千万级吞吐率。各应用强烈建议使用ICache接口编码设计，小数据时使用MemoryCache实现；数据增大（10万）以后，改用Redis实现，不需要修改业务代码。
-   Master最好不要做任何持久化工作，如RDB内存快照和AOF日志文件。如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次。
-   主从复制不要用图状结构，用单向链表结构更为稳定，即：Master <- Slave1 <- Slave2 <- Slave3...这样的结构方便解决单点故障问题，实现Slave对Master的替换。如果Master挂了，可以立刻启用Slave1做Master，其他不变。

**Memcached****的多线程** **vs. Redis****单线程**

Memcached内部是多线程的，而Redis是单线程的。

Redis的value的类型可以是list, hash, set。在实际应用场景中，很容易出现多个客户端对同一个key的这个复杂的value数据结构进行并发操作，如果是多线程，势必要引入锁，而锁却是性能杀手。

相比较而言，memcached只有简单的get/set/add操作，没有复杂数据结构，在互斥这个问题上，没有redis那么严重。



### **bloomfilter** 

对于存在性判断业务，采用Bloomfilter算法，在业务能容忍一定误判的前提下，可以大幅的降低内存占用。

bloomfilter利用bit数组来表示一个“存在”记录，优化了bitmap用一个bit记录存在误判率过高的问题，bloomfilter采用多个bit位表示记录是否存在，多个bit由相互独立的hash函数生成，如下图，只有多个bit均为1时才表示“存在”（x1，x2，y2），只要有一个bit为0表示“不存在”（y1），两个key可以共用槽位（如x1和x2共用第二个bit）。这种算法存在一定的误判率，可能一个元素虽然不在集合中，由于hash冲突k个bit都可能与其它key冲突，这个冲突概率与数组被置为1的比例相关，写入的key（bit置为1）越多冲突概率越高，理论上整个数组有一半bit位被置为1时，存储与误判处在一个较为理想的平衡点，即空间充分利用的同时误判可控。

![bloomfilter](assets/image2019-10-11_23-8-51.png)

根据不同的业务场景，bloomfilter使用场景也分为两类：

1.  非精确判断直接当存储用，比如阅读数统计中剔除重复阅读、推荐场景中剔除已经推荐过的Feed；
2.  精确判断当缓存用，典型架构为bloomfilter + Storage。

bloomfilter的目的主要挡掉大部分的“不存在”请求，降低请求延时和对storage的压力，比如Feed中大部分是没有赞过，可以使用bloomfilter挡掉不存在的请求；其次在很多存储组件也有大量使用，比如hbase、leveldb以降低对磁盘的空查询。

### **Docker**

容器的本质是被限制了的 Namespaces，cgroup，具有逻辑上独立文件系统、网络的一个进程。

**Namespaces****（视野隔离）**

Docker Engine 使用了以下 Linux 的隔离技术:

-   **The pid namespace**: 管理 PID 命名空间 (PID: Process ID).
-   **The net namespace**: 管理网络命名空间(NET: Networking).
-   **The ipc namespace**: 管理进程间通信命名空间(IPC: InterProcess Communication).
-   **The mnt namespace**: 管理文件系统挂载点命名空间 (MNT: Mount).
-   **The uts namespace**: Unix 时间系统隔离. (UTS: Unix Timesharing System).

通过这些技术，运行时的容器得以看到一个和宿主机上其他容器隔离的环境。

**Cgroups****（资源隔离）**

Docker 底层使用 groups 对进程进行 CPU，Mem，网络等资源的使用限制，从而实现在宿主机上的资源分配，不至于出现一个容器占用所有宿主机的 CPU 或者内存。

**UnionFS****（文件系统隔离）**